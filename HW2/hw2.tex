\title{Assignment 2: CS 763, Computer Vision}
\author{}
\date{Due: 17th February before 11:55 pm}

\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{hyperref}
%\usepackage{ulem}
\usepackage[margin=0.5in]{geometry}
\begin{document}
\maketitle

\textbf{Remember the honor code while submitting this (and every other) assignment. All members of the group should work on all parts of the assignment. We will adopt a \textbf{zero-tolerance policy} against any violation.}
\\
\\
\textbf{Submission instructions:} You should ideally type out all the answers in Word (with the equation editor) or using Latex. In either case, prepare a pdf file. Put the pdf file and the code for the programming parts all in one zip file. The pdf should contain the names and ID numbers of all students in the group within the header. The pdf file should also contain instructions for running your code. Name the zip file as follows: A2-IdNumberOfFirstStudent-IdNumberOfSecondStudent-IdNumberOfThirdStudent.zip. (If you are doing the assignment alone, the name of the zip file is A1-IdNumber.zip). Upload the file on moodle BEFORE 11:55 pm on 17th February. Late assignments will be assessed a penalty of 25\% per day late. Note that only one student per group should upload their work on moodle. Please preserve a copy of all your work until the end of the semester. 

\begin{enumerate}
\item Prove that collinearity of points is preserved under perspective projection. \textsf{[4 points]}

\begin{itemise}
\item Let

\item We have defined the concept of the Shannon entropy in class. Given a discrete random variable $X$ having probability mass function $P(X) = (p_1, p_2, ..., p_N)$, prove that $H(X) \geq 0$ where $H(X)$ is the Shannon entropy of $X$. Recall that $H(X) = -\sum\limits_{i=1}^N p_i \log p_i$ and that $\sum\limits_{i=1}^N p_i = 1$ and $\forall i, 0 \leq p_i \leq 1$. Also prove that a uniform distribution (\textit{ie} $\forall i, \textrm{ } p_i = \frac{1}{N}$) maximizes the Shannon entropy. To this end, find a stationary point of $J(X) = H(X) - \lambda (\sum\limits_{i=1}^N p_i - 1)$ where $\lambda$ is a Lagrange multiplier to impose the hard constraint that the probabilities all sum up to 1.  \textsf{[2+4 = 6 points]}

\item This is a straightforward exercise to make sure you understand the basic update equations in the Horn-Shunck algorithm for optical flow. As seen in class, we seek to minimize the quantity $J(\{(u_{ij},v_{ij})\})$ w.r.t. the optical flow vectors $(u_{i,j},v_{i,j})$ at all pixels $(i,j)$, where $J(\{(u_{i,j},v_{i,j})\}) = \sum\limits_{i=1}^N \sum\limits_{j=1}^N (I_{x;i,j} u_{i,j} + I_{y;i,j} v_{i,j} + I_{t;i,j} )^2 + \lambda ((u_{i,j+1} - u_{i,j})^2 + (u_{i+1,j} - u_{i,j})^2 + (v_{i,j+1} - v_{i,j})^2 + (v_{i+1,j} - v_{i,j})^2)$. Setting the partial derivatives w.r.t. $u_{k,l}$ and $v_{k,l}$ to 0, prove that 
\begin{eqnarray}
u_{k,l} = \bar{u}_{k,l} - \dfrac{I_{x;k,l} (I_{x;k,l} \bar{u}_{k,l} + I_{y;k,l} \bar{v}_{k,l} + I_{t;k,l})}{I^2_{x;k,l} + I^2_{y;k,l} + 4 \lambda} \\ 
v_{k,l} = \bar{v}_{k,l} - \dfrac{I_{y;k,l} (I_{x;k,l} \bar{u}_{k,l} + I_{y;k,l} \bar{v}_{k,l} + I_{t;k,l})}{I^2_{x;k,l} + I^2_{y;k,l} + 4 \lambda}
\end{eqnarray}
where $\bar{u}_{k,l}$ and $\bar{v}_{k,l}$ are as defined in the lecture slides. \textsf{[4 points]}

\item You know that both the Horn-Shunck as well as Lucas-Kanade methods bank on the brightness constancy assumption. Given a pair of images, let us suppose that this assumption holds good for most physically corresponding pixels, but not for some $p \%$ of the pixels. Briefly explain how you will modify the Horn-Shunck method and Lucas-Kanade method to deal with this. \textsf{[4 points]}

\item In this assignment, you will build up on the previous assignment to estimate the homography between two images. This time, you should use the RanSaC algorithm to estimate the homography in order to make the estimate resistant to the presence of incorrect point correspondences. The code for RanSaC for various problems including estimation of homographies is available. You should work with the images in the folder and also on any one pair of pictures of an approximately planar scene taken with a real camera. (You should acquire these images yourself and make sure they have small non-overlapping areas to make this more interesting.) Also, in each case, you should warp one of the images so that it aligns with the other one. However, this time you should \emph{not} crop the image so that no parts of either image are deleted, and a true mosaic may be obtained. While you should use the RanSaC code as is, I recommend stepping through it to get a feel for what is going on inside. Display all the resulting mosaics in your report. State the number of iterations and all thresholds you used. \textsf{[5 points]}

\item Here we will register a flash and a no-flash image pair using the joint entropy criterion we studied in class. Download the flash and no-flash images from. Convert both images to gray-scale. The flash image and the no-flash image have different image intensities at many places, and the no-flash image is distinctly noisier. Rotate the given no-flash image counter-clockwise by 28.5 degrees, translate it by -6 pixels in the X direction, and add Gaussian noise of standard deviation 5 (on a 0-255 scale). Note that the rotation must be applied about the center of the image. Set negative-valued pixels to 0 and pixels with value more than 255 to 255. Now perform a brute-force search to find the angle $\theta$ and translation $t_x$ to optimally align the modified no-flash image with the flash image, so as to minimize the joint entropy. The range for $\theta$ is between -60 and +60 in steps of 1 degree, and the range for $t_x$ should be between -12 and +12 in steps of 1. Compute the joint entropy using a bin-size of 10 for both intensities. Plot the joint entropy as a function of $\theta$ and $t_x$ using the surf and imshow commands of MATLAB. Also, determine a scenario where the images are obviously misaligned but the joint entropy is (falsely and undesirably) lower than the `true' minimum. Again, display the joint entropy as mentioned before. Include all plots in your report. \textsf{[7 points]} 

\end{enumerate}
\end{document}